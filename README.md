# WEB CRAWLER
## A set of programs that emulate some functions of a search engine

### Implementation:

##### We have implemented a web crawler which asks the user for an intial webpage and then crawls through that webpage and its internal links.
##### It performs multiple searches and stores its data into a sql database(spider.sqllite)
##### Ranked the search results using the Page Rank algorithm to rank the most relevant search result
##### It then creates a visualization using a json file (which has all the data dumped into it), The result is a Node Web.

#### Please check the poster file in the Repository for more information


### Visualization:

![Webpage_graph](https://user-images.githubusercontent.com/69566994/149311366-65ab9cc7-0fa9-4367-8195-59bca4f2423d.png)


#### Notes:
- If you want to restart the Page Rank calculations without re-spidering the web pages, you can use spreset.py
- No future mass commits to main are allowed

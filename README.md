# WEB CRAWLER
## A set of programs that emulate some functions of a search engine

### Implementation:

##### We have implemented a web crawler which asks the user for an intial webpage and then crawls through that webpage and its internal links.
##### It performs multiple searches and stores its data into a sql database(spider.sqllite)
##### Ranked the search results using the Page Rank algorithm to rank the most relevant search result
##### It then creates a visualization using a json file (which has all the data dumped into it), The result is a Node Web.

#### Please check the poster file in the Repository for more information


### Visualization:

Node Web - https://uofwaterloo-my.sharepoint.com/:i:/r/personal/s29zafar_uwaterloo_ca/Documents/Visualization_Results/Webpage_graph.png?csf=1&web=1&e=3SAGyh

#### Notes:
- If you want to restart the Page Rank calculations without re-spidering the web pages, you can use spreset.py
- No future mass commits to main are allowed
